# DLMDSML01 - Machine Learning

## Q & A - Sessions

### Introduction to Machine-Learning & Optimization

<details>
           <summary>Machine Learning Applications</summary>
           <p> 01b_ml_applications.ipynb (last update: 2021-08-22) </p>
</details>

<details>
           <summary>Our First Machine Learning Model</summary>
           <p> 01_intro_to_ml.ipynb (last update: 2021-08-22) </p>
</details>

<details>
           <summary>Optimization Algorithms in Machine Learning and Beyond</summary>
           <p> 02_optimization_algorithms.ipynb (last update: 2021-03-23) </p>
</details>

### Regression & Classification

<details>
           <summary>Regression</summary>
           <p> 03_regression.ipynb (last update: 2021-04-06)</p>
</details>

<details>
           <summary>Hands-On Classification</summary>
           <p> not yet prepared (last update: xx-xx-xx)</p>
</details>


<details>
           <summary>Multiclass Classification</summary>
           <p> multiclass_classification.ipynb (last update: 2021-02-09): We discuss how to generalize a classification problem to a multiclass classification problem. First of all, we show how to transform a logistic regression model into a multinomial logistic regression model. Then we show, with the use of the Iris dataset, how to generalize the sklearn classification algorithms to multiclass problems. After an outlook into multiclass performance metrics, like a multiclass confusion matrix, we discuss so-called meta-estimators available in *sklearn.multiclass* which help to increase accuracy and runtime performance of the classifiers . </p>
</details>


### Clustering

<details>
           <summary>Hands-On Clustering</summary>
           <p> 02_clustering.ipynb (last update: 2021-04-26): We analyze clustering algorithms both from a practical and a theoretical perspective. We go into detail of different clustering approaches, like k-means clustering, Gaussian mixture models, DBSCAN and hierachical clustering. In order to gain insights into the theoretical aspects of clustering we discuss the concept of similarity measures and define metrics to measure the quality of clustering methods. Finally we evaluate our techniques on a clustering use case.</p>
</details>

<details>
           <summary>Hands-On Clustering - Part II</summary>
           <p> 02b_clustering.ipynb (last update: 2021-05-04)</p>
</details>

### Support Vector Machines

<details>
           <summary>Hands-On Support Vector Machines</summary>
           <p> 04_support_vector_machines.ipynb (last update: 2021-06-08)</p>
</details>

### Decision Trees and Ensemble Methods

<details>
           <summary>Decision Trees and Random Forests </summary>
           <p> 05_decision_trees_and_random_forests.ipynb (last update: 2021-06-22)</p>
</details>

<details>
           <summary>Boosting Methods</summary>
           <p> 09_boosting_methods.ipynb (last update: 2021-07-06): We deepen our understanding of random forest algorithms, namely how boosting trees work. After discussing an analytical example we go over to the scikit learn's implementation of boosted trees. We also discuss most recent algorithms, as XGBoost, LightGBM and CatBoost.</p>
</details>


### Genetic Algorithms (GAs)

<details>
           <summary>Theory and Concepts</summary>
           <p> Q_A_genetic_algorithms_theory.ipynb (last update: 2021-07-20): Based on *Haupt & Haupt, Practical Genetic Algorithms (2004)* we discuss how to approach GAs both for binary as well as continuous problems. We try to understand how to encode variables, find the initial population, perform the natural selection process, discuss mating/crossover strategies and mutation strategies until convergence is reached.</p>
</details>

<details>
           <summary>Applications</summary>
           <p> Q_A_genetic_algorithms_applications.ipynb: The knapsack problem and the traveling salesman problem. (last update: 2021-07-20)</p>
</details>


### Additional Material

<details>
           <summary>Performance Metrics </summary>
           <p> performance_measures.ipynb (last update: 2020-12-22)
               We discuss how to evaluate the performance of a machine-learning algorithm, both for
supervised and unsupervised tasks. Jupyter notebook exploring
the individual performance measures from the *sklearn.metrics* functions.
 </p>
         </details>

 <details>
           <summary>Recommendation Systems </summary>
           <p> recommendation_systems.ipynb` (last update: 2021-01-05):
               We discuss the basic principles of how to implement recommendation systems. For the MovieLens dataset we build up a first, simple user-based collaborative filtering movie recommendation system.
 </p>
         </details>

<details>
           <summary>Machine Learning and Parallel Computing </summary>
           <p> multiclass_classification.ipynb (last update: 2021-02-23):
               We show on a simple example how easy it is to parallelize a for-loop in python (see main.py and main_multi.py). We then turn to parallelizable tasks in Machine Learning, the difference between data and model parallelization, GPU usage and cloud computing.
 </p>
         </details>

<details>
           <summary>Open Questions </summary>
           <p> open_questions.ipynb (last update: 2021-08-10):
               Open questions on Machine Learning, where you can test your knowledge and understanding. 
 </p>
         </details>
