{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-Learning and Parallel Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:22:55.480088Z",
     "start_time": "2021-02-23T17:22:55.466097Z"
    }
   },
   "source": [
    "Main Source: \n",
    "    \n",
    "*    Aurélien Géron - Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems-O’Reilly Media (2019) \n",
    "\n",
    "*    Scikit-Learn Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Computing is an important technique when building up machine-learning models: If we\n",
    "have a computation-intensive task, it will only run on one core, even if our\n",
    "computer has multiple cores. So we will waste a lot of computational power. \n",
    "\n",
    "In machine-learning there exist many tasks which can be parallelized. Just think of **hyperparameter testing** or **cross-validation**. Using all available cores will significantly reduce the amount of computation time used to evaluate the hyperparameter space.\n",
    "\n",
    "As an example sklearns `GridSearch` class: \n",
    "\n",
    "```python\n",
    "sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
    "```\n",
    "\n",
    "Here we have a parameter for parallelization:\n",
    "```python\n",
    "n_jobs: int, default=None\n",
    "Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.\n",
    "```\n",
    "For parallelization sklearn uses the `joblib` [library](https://joblib.readthedocs.io/en/latest/parallel.html#thread-based-parallelism-vs-process-based-parallelism).\n",
    "\n",
    "(Additional notes for software developers: There is also the possibility to use `OpenMP`or also parallelize `numpy`or `scipy` routines.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forests** are also perfectly parallelizable because each tree is trained on a subsample of data and features and do not depend on the other trees:\n",
    "\n",
    "```python\n",
    "class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parallelization and Data Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flavors of Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gradient Descent (GD)* is a very generic optimization algorithm which uses parameters iteratively in order to minimize a cost function. (Side node: An important parameter in GD is the size of the steps, the *learning rate*.)\n",
    "\n",
    "*Stochastic gradient descent* just picks a random instance in the training set at every step and computes the gradients baseld only on that single instance. Obviously this makes the algorithm much faster. However this algorithm is much less regular than GD. \n",
    "\n",
    "Implementation in sklearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(n_iter = 50, penalty = None, eta0 = 0.1)\n",
    "sgd_reg.fit(X,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T07:24:35.644301Z",
     "start_time": "2021-02-22T07:24:35.640302Z"
    }
   },
   "source": [
    "## Mini-Batch GD and Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Mini-batch Gradient Descent (GD)* computes the gradients on small random sets of instances called *mini-batches*. The main advantage of Mini-batch GD is that you can get a performance boost from hardware opmtimization of matrix operations, especially when using GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*     **Data parallelization**: Compute multiple gradients at once, so to run a training step simultaneously on all devides using a different mini-batch for each and then aggregate the gradients to pudate the model parameters. Here one has the possibility of *synchronous updates* and *asynchronous updates*. \n",
    "\n",
    "\n",
    "*     **Model parallelization**: Parallelize the computation of a single gradient. This requires chopping your model into separate chunks and running each chunk on a different device. Model parallelism turns out to be pretty tricky for many models, like neural networks. For random forests it is straightforward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Ways to Parallelize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we have just considered the option to use the cores on a single computer. We could also use\n",
    "\n",
    "*    **One or multiple CPUSs**\n",
    "\n",
    "\n",
    "*    **One or multiple GPUs**, which can further speed up the computation - which you can buy or just use a hosting service (Amazon AWS, Microsoft AZURE, Google Cloud GCP)\n",
    "\n",
    "     There is a good [blog](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/) to help choosing the right GPU.\n",
    "     \n",
    "     There also exist special processors, so-called *tensor processing units (TPUs)*, specialized for Machine Learning that are faster than GPUs for many ML tasks. They are developed by Google especially for usage with TensorFlow.\n",
    "     \n",
    "     \n",
    "*    **Cluster computing on multiple nodes**: A *cluster* is composed of one or more servers typically spread cross several machines (called *nodes*). Each server belongs to a *job*. Each node receives and completes many small tasks reporting the result to a central server which integrates the results into the overall solution. Each of the nodes has its own local memory. Since information is exchanged through a network, care must be taken in order to select the amount of information that is passed (in order not to lower computational performance). \n",
    "\n",
    "\n",
    "*    **Cloud Computing**: Platforms such as Amazon AWS, Google Cloud, and Microsoft Azure make\n",
    "it easy to rent large (or small) numbers of machines for short-term (or longterm)\n",
    "jobs. They provide you with the ability to get access to exactly the right\n",
    "computing resources when you need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizable Tasks in Machine-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Grid Search:** \n",
    "   *   k-Fold Cross-Validation, \n",
    "   *   Hyperparameter Testing\n",
    "\n",
    "\n",
    "* **Predictions on many instances**\n",
    "\n",
    "\n",
    "* **Some algorithms like random forests, mini-batch gradient descent**\n",
    "\n",
    "\n",
    "* **Neural networks** are said to be embarrassingly parallel, which means computations in neural networks can be executed in parallel easily and they are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Libraries for Large-Scale Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to break up the computation into several chunks and run them in parallel across multiple CPUs or GPUs - or also across hundreds of servers. *You can train a network with millions of parameters on a training set composed of billions of instances with millions of features each.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](https://pytorch.org/): primarily developed by Facebook's AI Research Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow](https://www.tensorflow.org/): developed by the Google Brain team "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dask](https://tutorial.dask.org/00_overview.html) is a parallel computing library that scales the existing Python libraries. (Interesting here are also the description of use cases)\n",
    "\n",
    "*(Dask provides ways to scale Pandas, Scikit-Learn, and Numpy workflows more natively, with minimal rewriting. It integrates well with these tools so that it copies most of their API and uses their data structures internally. Moreover, Dask is co-developed with these libraries to ensure that they evolve consistently, minimizing friction when transitioning from a local laptop, to a multi-core workstation, and then to a distributed cluster. Analysts familiar with Pandas/Scikit-Learn/Numpy will be immediately familiar with their Dask equivalents, and have much of their intuition carry over to a scalable context.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Rapids](https://rapids.ai/) builds Machine Learning algorithms on GPUs. It uses CUDA: cuML https://github.com/rapidsai/cuml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA** is a parallel computing platform and programming model that is developed by Nvidia offers its way of task parallel and data parallel computing models to give many options for a problem to be solved. You can push different jobs to same GPU concurrently or compute a one data parallel job with using all its resources, to finish it much quicker than a CPU does.\n",
    "CUDA enables developers to speed up compute-intensive applications by harnessing the power of GPUs for the parallelizable part of the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms with GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **XGBoost**:\n",
    "\n",
    "     XGBoost supports fully distributed GPU training using Dask\n",
    "\n",
    "\n",
    "\n",
    "*    **LightGBM**:\n",
    "\n",
    "     GPU version available: https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From stackoverflow: Will you add GPU support in scikit-learn?*\n",
    "\n",
    "*No, or at least not in the near future. The main reason is that GPU support will introduce many software dependencies and introduce platform specific issues. scikit-learn is designed to be easy to install on a wide variety of platforms. Outside of neural networks, GPUs don’t play a large role in machine learning today, and much larger gains in speed can often be achieved by a careful choice of algorithms.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    [IPython](https://github.com/ipython/ipyparallel) IPython.parallel. \n",
    "\n",
    "     ipyparallel is a Python package and collection of scripts for controlling clusters for Jupyter\n",
    "\n",
    "```python\n",
    "from IPython import parallel\n",
    "engines = parallel.Client()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization for Hyperparameter Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Put all your hyperparameters in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test['learning_rate'] = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "param_test['n_estimators']  = [50, 100, 150, 200]\n",
    "\n",
    "\n",
    "configuations = [[0.01, 50, ...], [0.01, 100, ...], [], []]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Get Parameter Configurations.\n",
    "configurations = []\n",
    "for a in param_test['learning_rate']:\n",
    "    for b in param_test['n_estimators']:\n",
    "        for c in param_test['max_depth']:\n",
    "            for d in param_test['min_child_weight']:\n",
    "                for e in param_test['gamma']:\n",
    "                    for f in param_test['subsample']:\n",
    "                        for g in param_test['colsample_bytree']:\n",
    "                            for h in param_test['nthread']:\n",
    "                                for i in param_test['scale_pos_weight']:\n",
    "                                    for j in param_test['seed']:\n",
    "                                        param = {'learning_rate'    : a,\n",
    "                                                 'n_estimators'     : b,\n",
    "                                                 'max_depth'        : c,\n",
    "                                                 'min_child_weight' : d,\n",
    "                                                 'gamma'            : e,\n",
    "                                                 'subsample'        : f,\n",
    "                                                 'colsample_bytree' : g,\n",
    "                                                 'nthread'          : h,\n",
    "                                                 'scale_pos_weight' : i,\n",
    "                                                 'seed'             : j,\n",
    "                                                 'objective'        : 'reg:squarederror'} \n",
    "                                        configurations.append(param)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Use the joblib library to parallelize the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T12:34:13.643139Z",
     "start_time": "2021-02-23T12:34:13.632136Z"
    }
   },
   "source": [
    "```python\n",
    "from joblib import Parallel, delayed\n",
    "#with parallel_backend('multiprocessing'): #threading\n",
    "models, params, selections, val_errors, trn_errors = zip(*Parallel(\n",
    "                                                                n_jobs=-1,\n",
    "                                                                verbose=50,\n",
    "                                                                backend=\"multiprocessing\"\n",
    "                                                            )(\n",
    "                                                                map(delayed(XGB_fit), \n",
    "                                                                [train]*len(configurations), \n",
    "                                                                [val]*len(configurations), \n",
    "                                                                configurations, \n",
    "                                                                np.repeat(model_type, len(configurations)))\n",
    "                                                            )\n",
    "                                                        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joblib](https://joblib.readthedocs.io/en/latest/parallel.html#thread-based-parallelism-vs-process-based-parallelism) user manual \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-In Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the underlying implementation uses joblib, the number of workers (threads or processes) that are spawned in parallel can be controlled via the n_jobs parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T12:34:35.696412Z",
     "start_time": "2021-02-23T12:34:35.688414Z"
    }
   },
   "source": [
    "```python\n",
    "model_cv = GridSearchCV(model, param_grid, \n",
    "                        scoring=scoring, \n",
    "                        n_jobs=-1, \n",
    "                        cv=cross_val, \n",
    "                        verbose=10, \n",
    "                        return_train_score=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dockerfile, Kubernetes (not yet finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kubernetes** is an “open-source container-orchestration system for automating deployment, scaling and management of **containerized** applications”.\n",
    "\n",
    "**Containerisation** is an alternative to full virtualisation, where an application runs in a container with its own operating system. This enables the developers to run the application on any environment without having to worry about dependencies. \n",
    "\n",
    "By containerising the fitting of each model, one can run an entire ML gridsearch in parallel on Kubernetes as the containers are independent from each other.\n",
    "\n",
    "The ability to run models in parallel results in a significant boost in term of performance compared to a sequential approach, as well as allowing us to manage resources more efficiently. Kubernetes allows the resource management team to allocate different amount of memory and CPU for each container, which means we can allocate more resources for complex algorithms like XGBoost that rely on multi-threading compared to single threaded algorithms like LogisticRegression from sklearn. This allows us to make the most of our resources, keep the cost down, and run more pipelines simultaneously."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "301.698px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
